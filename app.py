"""
Open-source RAG chatbot for cricketer PDFs.

Features:
- Loads all PDFs from ./data
- Splits documents into overlapping chunks
- Builds/loads FAISS vector index with sentence-transformers embeddings
- Retrieves relevant chunks and answers with a local HuggingFace LLM
- Maintains chat history in memory
- Returns a strict fallback message when information is not found

Run:
  streamlit run app.py
"""

from __future__ import annotations

import os
import re
import json
from pathlib import Path
from typing import List

import streamlit as st
import torch
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import HuggingFacePipeline
from langchain_community.vectorstores import FAISS
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

try:
    # LangChain <=0.2 style imports
    from langchain.chains import RetrievalQA
    from langchain.memory import ConversationBufferMemory
    from langchain.prompts import PromptTemplate
    from langchain.text_splitter import RecursiveCharacterTextSplitter
except ModuleNotFoundError:
    # LangChain >=1.0 moved legacy chain APIs to langchain_classic
    from langchain_classic.chains import RetrievalQA
    from langchain_classic.memory import ConversationBufferMemory
    from langchain_classic.prompts import PromptTemplate
    from langchain_text_splitters import RecursiveCharacterTextSplitter


FALLBACK_ANSWER = "I could not find this information in the provided documents."
DEFAULT_MODEL = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"


def get_env_str(name: str, default: str) -> str:
    value = os.getenv(name, "").strip()
    return value or default


def get_env_int(name: str, default: int) -> int:
    value = os.getenv(name, "").strip()
    if not value:
        return default
    try:
        return int(value)
    except ValueError:
        return default


def load_pdf_documents(data_dir: Path) -> List:
    """Load all PDFs from the data directory."""
    if not data_dir.exists():
        raise FileNotFoundError(
            f"Data directory not found: {data_dir}. Create it and add cricketer PDFs."
        )

    pdf_files = sorted(data_dir.glob("*.pdf"))
    if not pdf_files:
        raise FileNotFoundError(
            f"No PDF files found in {data_dir}. Add at least one cricketer PDF."
        )

    docs = []
    for pdf in pdf_files:
        loader = PyPDFLoader(str(pdf))
        docs.extend(loader.load())
    return docs


def list_available_players(data_dir: str) -> List[str]:
    """Infer player names from PDF filenames in data directory."""
    path = Path(data_dir)
    if not path.exists():
        return []
    names = []
    for pdf in sorted(path.glob("*.pdf")):
        names.append(pdf.stem.replace("_", " "))
    return names


def load_player_metadata(data_dir: str) -> dict:
    """Load optional player metadata generated by dataset builder."""
    path = Path(data_dir) / "player_metadata.json"
    if not path.exists():
        return {}
    try:
        return json.loads(path.read_text(encoding="utf-8"))
    except Exception:
        return {}


def split_documents(documents: List) -> List:
    """Split long pages into overlapping chunks for retrieval."""
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ". ", " ", ""],
    )
    return splitter.split_documents(documents)


@st.cache_resource(show_spinner=False)
def get_embeddings() -> HuggingFaceEmbeddings:
    """Load open-source embedding model."""
    return HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )


@st.cache_resource(show_spinner=False)
def build_or_load_vectorstore(data_dir: str, index_dir: str) -> FAISS:
    """Create FAISS index from PDFs or load persisted index."""
    data_path = Path(data_dir)
    index_path = Path(index_dir)
    embeddings = get_embeddings()

    if (index_path / "index.faiss").exists() and (index_path / "index.pkl").exists():
        return FAISS.load_local(
            folder_path=str(index_path),
            embeddings=embeddings,
            allow_dangerous_deserialization=True,
        )

    documents = load_pdf_documents(data_path)
    chunks = split_documents(documents)

    vectorstore = FAISS.from_documents(chunks, embeddings)
    index_path.mkdir(parents=True, exist_ok=True)
    vectorstore.save_local(str(index_path))
    return vectorstore


@st.cache_resource(show_spinner=False)
def load_local_llm(model_id: str) -> HuggingFacePipeline:
    """Load local open-source LLM through Transformers."""
    tokenizer = AutoTokenizer.from_pretrained(model_id)

    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch_dtype,
        device_map="auto" if torch.cuda.is_available() else None,
    )

    generator = pipeline(
        task="text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=get_env_int("MAX_NEW_TOKENS", 192),
        do_sample=False,
        temperature=0.0,
        repetition_penalty=1.05,
        return_full_text=False,
    )
    return HuggingFacePipeline(pipeline=generator)


def get_qa_prompt() -> PromptTemplate:
    """Prompt that strictly limits answers to retrieved context."""
    template = """You are a cricket information assistant.
Use ONLY the context below to answer the user's question.
If the answer is missing, unclear, or not explicitly stated in the context,
respond EXACTLY with this sentence:
I could not find this information in the provided documents.
Only answer for the selected player mentioned in the question.
If the question asks about one format (e.g., ODI), do not include other formats unless asked.
Do not infer or estimate missing values.

Context:
{context}

Question:
{question}

Answer:"""
    return PromptTemplate(template=template, input_variables=["context", "question"])


def build_chain(model_id: str, retriever):
    """Build retrieval QA chain with conversational memory."""
    if "memory" not in st.session_state:
        st.session_state.memory = ConversationBufferMemory(
            memory_key="chat_history",
            input_key="query",
            output_key="result",
            return_messages=True,
        )

    llm = load_local_llm(model_id)
    prompt = get_qa_prompt()

    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        memory=st.session_state.memory,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt},
    )


def answer_query(query: str, qa_chain, retriever) -> tuple[str, List]:
    """Retrieve first, then answer. Enforce exact fallback behavior."""
    if hasattr(retriever, "get_relevant_documents"):
        docs = retriever.get_relevant_documents(query)
    else:
        docs = retriever.invoke(query)
    if not docs:
        return FALLBACK_ANSWER, []

    if hasattr(qa_chain, "invoke"):
        result = qa_chain.invoke({"query": query})
    else:
        result = qa_chain({"query": query})
    answer = result.get("result", "").strip()
    sources = result.get("source_documents", [])

    if not answer:
        return FALLBACK_ANSWER, sources

    # Hard guard to keep fallback text exact when model indicates missing info.
    lowered = answer.lower()
    if "could not find" in lowered or "not found" in lowered or "not provided" in lowered:
        return FALLBACK_ANSWER, sources

    # Guardrail: for stat-heavy questions, reject answers containing numbers
    # not present in retrieved context to reduce hallucinated figures.
    stat_keywords = [
        "match",
        "matches",
        "run",
        "runs",
        "wicket",
        "wickets",
        "average",
        "strike rate",
        "century",
        "fifty",
        "test",
        "odi",
        "t20",
    ]
    is_stat_query = any(k in query.lower() for k in stat_keywords)
    if is_stat_query:
        context_text = " ".join(getattr(d, "page_content", "") for d in docs)
        ctx_nums = {
            n.replace(",", "")
            for n in re.findall(r"\d+(?:,\d{3})*(?:\.\d+)?", context_text)
        }
        ans_nums = {
            n.replace(",", "") for n in re.findall(r"\d+(?:,\d{3})*(?:\.\d+)?", answer)
        }
        if ans_nums and not ans_nums.issubset(ctx_nums):
            return FALLBACK_ANSWER, sources

    return answer, sources


def reset_chat():
    """Clear chat UI and memory."""
    st.session_state.messages = []
    if "memory" in st.session_state:
        st.session_state.memory.clear()


def submit_question(question: str, selected_player: str, qa_chain, retriever) -> None:
    """Submit one user question and append user/assistant messages."""
    question = question.strip()
    if not question:
        return

    final_query = f"For player {selected_player}: {question}"
    user_content = f"{selected_player}: {question}"

    st.session_state.messages.append({"role": "user", "content": user_content})
    with st.chat_message("user"):
        st.markdown(user_content)

    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            answer, source_docs = answer_query(final_query, qa_chain, retriever)
        st.markdown(answer)

    st.session_state.messages.append(
        {"role": "assistant", "content": answer}
    )


def main():
    st.set_page_config(page_title="Cricket RAG Chatbot", page_icon="üèè", layout="wide")
    st.title("üèè Cricket RAG Chatbot")
    st.caption("Ask questions about IPL cricketers using your local PDF knowledge base.")

    model_id = get_env_str("HF_MODEL_ID", DEFAULT_MODEL)
    data_dir = get_env_str("DATA_DIR", "data")
    index_dir = get_env_str("INDEX_DIR", "faiss_index")
    top_k = get_env_int("TOP_K", 4)

    try:
        with st.spinner("Loading vector index..."):
            vectorstore = build_or_load_vectorstore(data_dir=data_dir, index_dir=index_dir)
        retriever = vectorstore.as_retriever(search_kwargs={"k": top_k})

        with st.spinner("Loading local LLM..."):
            qa_chain = build_chain(model_id=model_id, retriever=retriever)
    except Exception as exc:
        st.error(f"Initialization failed: {exc}")
        st.stop()

    if "messages" not in st.session_state:
        st.session_state.messages = []

    players = list_available_players(data_dir)
    metadata = load_player_metadata(data_dir)
    if not players:
        st.warning("No player PDFs found in `data/`. Add PDFs and rerun.")
        st.stop()

    with st.sidebar:
        st.subheader("Chat Settings")
        selected_player = st.selectbox(
            "Select player",
            options=players,
            index=0,
        )
        st.write(f"Loaded player PDFs: {len(players)}")
        if st.button("Reset Chat", use_container_width=True):
            reset_chat()
            st.rerun()

        st.markdown("**Quick prompts**")
        st.caption("Click one to send it instantly.")
        quick_prompts = [
            "What is his playing role?",
            "Summarize his career highlights.",
            "How many international matches has he played?",
            "What are his ODI/Test/T20 stats?",
        ]
        for idx, prompt in enumerate(quick_prompts):
            if st.button(prompt, key=f"quick_{idx}", use_container_width=True):
                st.session_state.quick_query = prompt

    st.info(f"Currently chatting about: **{selected_player}**")
    meta = metadata.get(selected_player, {})
    if meta:
        c1, c2 = st.columns([1, 2])
        with c1:
            img = meta.get("image_path", "")
            if img and Path(img).exists():
                st.image(img, caption=selected_player, use_container_width=True)
        with c2:
            st.markdown("**Basic Info**")
            st.write(f"Age: {meta.get('age', 'N/A') or 'N/A'}")
            st.write(f"IPL Team: {meta.get('ipl_team', 'N/A') or 'N/A'}")
            st.write(f"Country: {meta.get('country', 'N/A') or 'N/A'}")
            st.write(f"Role: {meta.get('role', 'N/A') or 'N/A'}")
            st.write(f"Matches: {meta.get('matches', 'N/A') or 'N/A'}")
            st.write(f"Runs: {meta.get('runs', 'N/A') or 'N/A'}")
            st.write(f"Wickets: {meta.get('wickets', 'N/A') or 'N/A'}")

    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    query = st.chat_input(f"Ask about {selected_player}")

    quick_query = st.session_state.pop("quick_query", None)
    if quick_query:
        submit_question(quick_query, selected_player, qa_chain, retriever)

    if query:
        submit_question(query, selected_player, qa_chain, retriever)


if __name__ == "__main__":
    # Optional for some CPU-only environments
    os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
    main()
